{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f75e9c",
   "metadata": {},
   "source": [
    "## The analysis is divided into three parts:\\n\",\"1. Training SVM with all features\\n\",\"2. Comparing different kernels\\n\",\"3. Visualizing decision boundaries using selected features\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9943375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de4fd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28392ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   duration protocol_type service flag  src_bytes  dst_bytes  land  \\\n",
      "0         0           tcp    http   SF        181       5450     0   \n",
      "1         0           tcp    http   SF        239        486     0   \n",
      "2         0           tcp    http   SF        235       1337     0   \n",
      "3         0           tcp    http   SF        219       1337     0   \n",
      "4         0           tcp    http   SF        217       2032     0   \n",
      "\n",
      "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
      "0               0       0    0  ...                   9   \n",
      "1               0       0    0  ...                  19   \n",
      "2               0       0    0  ...                  29   \n",
      "3               0       0    0  ...                  39   \n",
      "4               0       0    0  ...                  49   \n",
      "\n",
      "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0                     1.0                     0.0   \n",
      "1                     1.0                     0.0   \n",
      "2                     1.0                     0.0   \n",
      "3                     1.0                     0.0   \n",
      "4                     1.0                     0.0   \n",
      "\n",
      "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                         0.11                          0.0   \n",
      "1                         0.05                          0.0   \n",
      "2                         0.03                          0.0   \n",
      "3                         0.03                          0.0   \n",
      "4                         0.02                          0.0   \n",
      "\n",
      "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                   0.0                       0.0                   0.0   \n",
      "1                   0.0                       0.0                   0.0   \n",
      "2                   0.0                       0.0                   0.0   \n",
      "3                   0.0                       0.0                   0.0   \n",
      "4                   0.0                       0.0                   0.0   \n",
      "\n",
      "   dst_host_srv_rerror_rate   label  \n",
      "0                       0.0  normal  \n",
      "1                       0.0  normal  \n",
      "2                       0.0  normal  \n",
      "3                       0.0  normal  \n",
      "4                       0.0  normal  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "        duration protocol_type service flag  src_bytes  dst_bytes  land  \\\n",
      "494015         0           tcp    http   SF        310       1881     0   \n",
      "494016         0           tcp    http   SF        282       2286     0   \n",
      "494017         0           tcp    http   SF        203       1200     0   \n",
      "494018         0           tcp    http   SF        291       1200     0   \n",
      "494019         0           tcp    http   SF        219       1234     0   \n",
      "\n",
      "        wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
      "494015               0       0    0  ...                 255   \n",
      "494016               0       0    0  ...                 255   \n",
      "494017               0       0    0  ...                 255   \n",
      "494018               0       0    0  ...                 255   \n",
      "494019               0       0    0  ...                 255   \n",
      "\n",
      "        dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "494015                     1.0                     0.0   \n",
      "494016                     1.0                     0.0   \n",
      "494017                     1.0                     0.0   \n",
      "494018                     1.0                     0.0   \n",
      "494019                     1.0                     0.0   \n",
      "\n",
      "        dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "494015                         0.01                         0.05   \n",
      "494016                         0.17                         0.05   \n",
      "494017                         0.06                         0.05   \n",
      "494018                         0.04                         0.05   \n",
      "494019                         0.17                         0.05   \n",
      "\n",
      "        dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "494015                  0.00                      0.01                   0.0   \n",
      "494016                  0.00                      0.01                   0.0   \n",
      "494017                  0.06                      0.01                   0.0   \n",
      "494018                  0.04                      0.01                   0.0   \n",
      "494019                  0.00                      0.01                   0.0   \n",
      "\n",
      "        dst_host_srv_rerror_rate   label  \n",
      "494015                       0.0  normal  \n",
      "494016                       0.0  normal  \n",
      "494017                       0.0  normal  \n",
      "494018                       0.0  normal  \n",
      "494019                       0.0  normal  \n",
      "\n",
      "[5 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "df= pd.read_csv(r\"C:\\Users\\17063\\Downloads\\kddcup99_csv 1.csv\")\n",
    "print(df.head())\n",
    "\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c393f6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and examining dataset...\n",
      "\n",
      "Dataset Information:\n",
      "--------------------------------------------------\n",
      "Number of rows: 494020\n",
      "Number of columns: 42\n",
      "\n",
      "Column names:\n",
      "1. duration\n",
      "2. protocol_type\n",
      "3. service\n",
      "4. flag\n",
      "5. src_bytes\n",
      "6. dst_bytes\n",
      "7. land\n",
      "8. wrong_fragment\n",
      "9. urgent\n",
      "10. hot\n",
      "11. num_failed_logins\n",
      "12. logged_in\n",
      "13. lnum_compromised\n",
      "14. lroot_shell\n",
      "15. lsu_attempted\n",
      "16. lnum_root\n",
      "17. lnum_file_creations\n",
      "18. lnum_shells\n",
      "19. lnum_access_files\n",
      "20. lnum_outbound_cmds\n",
      "21. is_host_login\n",
      "22. is_guest_login\n",
      "23. count\n",
      "24. srv_count\n",
      "25. serror_rate\n",
      "26. srv_serror_rate\n",
      "27. rerror_rate\n",
      "28. srv_rerror_rate\n",
      "29. same_srv_rate\n",
      "30. diff_srv_rate\n",
      "31. srv_diff_host_rate\n",
      "32. dst_host_count\n",
      "33. dst_host_srv_count\n",
      "34. dst_host_same_srv_rate\n",
      "35. dst_host_diff_srv_rate\n",
      "36. dst_host_same_src_port_rate\n",
      "37. dst_host_srv_diff_host_rate\n",
      "38. dst_host_serror_rate\n",
      "39. dst_host_srv_serror_rate\n",
      "40. dst_host_rerror_rate\n",
      "41. dst_host_srv_rerror_rate\n",
      "42. label\n",
      "\n",
      "First few rows:\n",
      "   duration protocol_type service flag  src_bytes  dst_bytes  land  \\\n",
      "0         0           tcp    http   SF        181       5450     0   \n",
      "1         0           tcp    http   SF        239        486     0   \n",
      "2         0           tcp    http   SF        235       1337     0   \n",
      "3         0           tcp    http   SF        219       1337     0   \n",
      "4         0           tcp    http   SF        217       2032     0   \n",
      "\n",
      "   wrong_fragment  urgent  hot  ...  dst_host_srv_count  \\\n",
      "0               0       0    0  ...                   9   \n",
      "1               0       0    0  ...                  19   \n",
      "2               0       0    0  ...                  29   \n",
      "3               0       0    0  ...                  39   \n",
      "4               0       0    0  ...                  49   \n",
      "\n",
      "   dst_host_same_srv_rate  dst_host_diff_srv_rate  \\\n",
      "0                     1.0                     0.0   \n",
      "1                     1.0                     0.0   \n",
      "2                     1.0                     0.0   \n",
      "3                     1.0                     0.0   \n",
      "4                     1.0                     0.0   \n",
      "\n",
      "   dst_host_same_src_port_rate  dst_host_srv_diff_host_rate  \\\n",
      "0                         0.11                          0.0   \n",
      "1                         0.05                          0.0   \n",
      "2                         0.03                          0.0   \n",
      "3                         0.03                          0.0   \n",
      "4                         0.02                          0.0   \n",
      "\n",
      "   dst_host_serror_rate  dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
      "0                   0.0                       0.0                   0.0   \n",
      "1                   0.0                       0.0                   0.0   \n",
      "2                   0.0                       0.0                   0.0   \n",
      "3                   0.0                       0.0                   0.0   \n",
      "4                   0.0                       0.0                   0.0   \n",
      "\n",
      "   dst_host_srv_rerror_rate   label  \n",
      "0                       0.0  normal  \n",
      "1                       0.0  normal  \n",
      "2                       0.0  normal  \n",
      "3                       0.0  normal  \n",
      "4                       0.0  normal  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "\n",
      "Using 'protocol_type' as attack type column\n",
      "\n",
      "Unique values in attack column: ['tcp' 'udp' 'icmp']\n",
      "\n",
      "Creating binary labels...\n",
      "\n",
      "DoS attacks found: 0 instances\n",
      "Non-DoS instances: 494020 instances\n",
      "\n",
      "Converting categorical variables...\n",
      "\n",
      "Feature matrix shape: (494020, 41)\n",
      "Target vector shape: (494020,)\n",
      "\n",
      "Splitting data...\n",
      "Scaling features...\n",
      "\n",
      "Error in data processing: could not convert string to float: 'normal'\n",
      "\n",
      "Program failed: could not convert string to float: 'normal'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'normal'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 151\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 151\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 139\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    136\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m17063\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mkddcup99_csv 1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Load and preprocess the data\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m X_train_scaled, X_test_scaled, y_train, y_test, scaler, label_encoders \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_process_kdd99\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model\u001b[39;00m\n\u001b[0;32m    142\u001b[0m model \u001b[38;5;241m=\u001b[39m train_and_evaluate_model(X_train_scaled, X_test_scaled, y_train, y_test)\n",
      "Cell \u001b[1;32mIn[19], line 101\u001b[0m, in \u001b[0;36mload_and_process_kdd99\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScaling features...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m--> 101\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X_train_scaled, X_test_scaled, y_train, y_test, scaler, label_encoders\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:878\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:914\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \n\u001b[0;32m    884\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    913\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 914\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    921\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py:745\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    743\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 745\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2063\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'normal'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def examine_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Examine the dataset structure and print relevant information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(\"\\nDataset Information:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Number of rows: {len(df)}\")\n",
    "        print(f\"Number of columns: {len(df.columns)}\")\n",
    "        print(\"\\nColumn names:\")\n",
    "        for i, col in enumerate(df.columns):\n",
    "            print(f\"{i+1}. {col}\")\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error examining dataset: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def load_and_process_kdd99(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the KDD99 dataset with flexible column handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Loading and examining dataset...\")\n",
    "        df = examine_dataset(file_path)\n",
    "        \n",
    "        # Check if 'attack_type' column exists or find alternative\n",
    "        attack_col = None\n",
    "        possible_attack_columns = ['attack_type', 'attack', 'class', 'label', 'type']\n",
    "        \n",
    "        for col in df.columns:\n",
    "            # Check for exact matches\n",
    "            if col.lower() in possible_attack_columns:\n",
    "                attack_col = col\n",
    "                break\n",
    "            # Check for partial matches\n",
    "            for possible_col in possible_attack_columns:\n",
    "                if possible_col in col.lower():\n",
    "                    attack_col = col\n",
    "                    break\n",
    "            if attack_col:\n",
    "                break\n",
    "        \n",
    "        if not attack_col:\n",
    "            print(\"\\nAvailable columns:\", df.columns.tolist())\n",
    "            raise ValueError(\"Could not find attack type column. Please specify the correct column name.\")\n",
    "        \n",
    "        print(f\"\\nUsing '{attack_col}' as attack type column\")\n",
    "        print(\"\\nUnique values in attack column:\", df[attack_col].unique())\n",
    "        \n",
    "        # Create binary labels (DoS vs Non-DoS)\n",
    "        print(\"\\nCreating binary labels...\")\n",
    "        # Adjust attack names based on your actual data\n",
    "        dos_attacks = ['back', 'land', 'neptune', 'pod', 'smurf', 'teardrop',\n",
    "                      'back.', 'land.', 'neptune.', 'pod.', 'smurf.', 'teardrop.',\n",
    "                      'DOS', 'dos', 'DoS']\n",
    "        df['binary_label'] = df[attack_col].apply(\n",
    "            lambda x: 1 if str(x).strip().lower() in [a.lower() for a in dos_attacks] else 0\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nDoS attacks found: {df['binary_label'].sum()} instances\")\n",
    "        print(f\"Non-DoS instances: {len(df) - df['binary_label'].sum()} instances\")\n",
    "        \n",
    "        # Convert categorical variables to numerical\n",
    "        print(\"\\nConverting categorical variables...\")\n",
    "        categorical_columns = ['protocol_type', 'service', 'flag']\n",
    "        # Only use categorical columns that exist in the dataset\n",
    "        categorical_columns = [col for col in categorical_columns if col in df.columns]\n",
    "        label_encoders = {}\n",
    "        \n",
    "        for column in categorical_columns:\n",
    "            label_encoders[column] = LabelEncoder()\n",
    "            df[column] = label_encoders[column].fit_transform(df[column])\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = df.drop([attack_col, 'binary_label'], axis=1)\n",
    "        y = df['binary_label']\n",
    "        \n",
    "        print(\"\\nFeature matrix shape:\", X.shape)\n",
    "        print(\"Target vector shape:\", y.shape)\n",
    "        \n",
    "        # Split the data\n",
    "        print(\"\\nSplitting data...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Scale the features\n",
    "        print(\"Scaling features...\")\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test, scaler, label_encoders\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError in data processing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def train_and_evaluate_model(X_train_scaled, X_test_scaled, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate the SVM model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"\\nTraining SVM model...\")\n",
    "        svm_model = SVC(kernel='rbf', random_state=42)\n",
    "        svm_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        print(\"Making predictions...\")\n",
    "        y_pred = svm_model.predict(X_test_scaled)\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "        \n",
    "        return svm_model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in model training: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Use the specific file path\n",
    "        file_path = r\"C:\\Users\\17063\\Downloads\\kddcup99_csv 1.csv\"\n",
    "        \n",
    "        # Load and preprocess the data\n",
    "        X_train_scaled, X_test_scaled, y_train, y_test, scaler, label_encoders = load_and_process_kdd99(file_path)\n",
    "        \n",
    "        # Train and evaluate the model\n",
    "        model = train_and_evaluate_model(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "        \n",
    "        return model, scaler, label_encoders\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nProgram failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "      \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00f03142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Preprocessing data...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'attack_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'attack_type'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Create binary labels\u001b[39;00m\n\u001b[0;32m     27\u001b[0m dos_attacks \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mback\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mland\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneptune\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpod\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmurf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteardrop\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 28\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattack_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m dos_attacks \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Convert categorical variables\u001b[39;00m\n\u001b[0;32m     31\u001b[0m categorical_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprotocol_type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mservice\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflag\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3631\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3630\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3631\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3632\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3633\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3634\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3635\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3636\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'attack_type'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Load and preprocess data (same as before)\n",
    "print(\"Loading dataset...\")\n",
    "cols = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', \n",
    "        'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "        'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "        'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "        'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
    "        'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "        'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "        'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "        'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "        'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack_type']\n",
    "\n",
    "df= pd.read_csv(r\"C:\\Users\\17063\\Downloads\\kddcup99_csv 1.csv\")\n",
    "\n",
    "print(\"Preprocessing data...\")\n",
    "# Create binary labels\n",
    "dos_attacks = ['back', 'land', 'neptune', 'pod', 'smurf', 'teardrop']\n",
    "df['binary_label'] = df['attack_type'].apply(lambda x: 1 if x in dos_attacks else 0)\n",
    "\n",
    "# Convert categorical variables\n",
    "categorical_columns = ['protocol_type', 'service', 'flag']\n",
    "label_encoders = {}\n",
    "for column in categorical_columns:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    df[column] = label_encoders[column].fit_transform(df[column])\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop(['attack_type', 'binary_label'], axis=1)\n",
    "y = df['binary_label']\n",
    "\n",
    "# Split and scale data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define kernels to test\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate models with different kernels\n",
    "for kernel in kernels:\n",
    "    print(f\"\\nTraining SVM with {kernel} kernel...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize and train model\n",
    "    svm_model = SVC(kernel=kernel, random_state=42)\n",
    "    svm_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = svm_model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate training time\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Store results\n",
    "    results[kernel] = {\n",
    "        'training_time': training_time,\n",
    "        'classification_report': classification_report(y_test, y_pred),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== COMPARISON OF SVM KERNELS ===\")\n",
    "for kernel in kernels:\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"Kernel: {kernel}\")\n",
    "    print(f\"Training Time: {results[kernel]['training_time']:.2f} seconds\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(results[kernel]['classification_report'])\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(results[kernel]['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cc30062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded successfully!\n",
      "Class distribution before balancing:\n",
      "0    494021\n",
      "Name: binary_label, dtype: int64\n",
      "Balancing dataset...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0 unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m min_class_count \u001b[38;5;241m=\u001b[39m class_counts\u001b[38;5;241m.\u001b[39mmin()\n\u001b[0;32m     49\u001b[0m class_0 \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msample(n\u001b[38;5;241m=\u001b[39mmin_class_count, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m class_1 \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary_label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_class_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m balanced_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([class_0, class_1])\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Update X and y after balancing\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5446\u001b[0m, in \u001b[0;36mNDFrame.sample\u001b[1;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[0;32m   5443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m     weights \u001b[38;5;241m=\u001b[39m sample\u001b[38;5;241m.\u001b[39mpreprocess_weights(\u001b[38;5;28mself\u001b[39m, weights, axis)\n\u001b[1;32m-> 5446\u001b[0m sampled_indices \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5447\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(sampled_indices, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[0;32m   5449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\sample.py:150\u001b[0m, in \u001b[0;36msample\u001b[1;34m(obj_len, size, replace, weights, random_state)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weights: weights sum to zero\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\n\u001b[0;32m    151\u001b[0m     np\u001b[38;5;241m.\u001b[39mintp, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    152\u001b[0m )\n",
      "File \u001b[1;32mmtrand.pyx:909\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading dataset...\")\n",
    "cols = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', \n",
    "        'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in',\n",
    "        'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations',\n",
    "        'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login',\n",
    "        'is_guest_login', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
    "        'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "        'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
    "        'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate',\n",
    "        'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
    "        'dst_host_rerror_rate', 'dst_host_srv_rerror_rate', 'attack_type']\n",
    "\n",
    "file_path = r\"C:\\Users\\17063\\Downloads\\kddcup99_csv 1.csv\"\n",
    "df = pd.read_csv(file_path, names=cols, header=None, low_memory=False)\n",
    "print(\"Dataset loaded successfully!\")\n",
    "\n",
    "# Normalize the 'attack_type' values to handle formatting issues\n",
    "df['attack_type'] = df['attack_type'].str.strip()\n",
    "\n",
    "# Create binary labels (DoS vs Non-DoS)\n",
    "dos_attacks = ['back.', 'land.', 'neptune.', 'pod.', 'smurf.', 'teardrop.']\n",
    "df['binary_label'] = df['attack_type'].apply(lambda x: 1 if x in dos_attacks else 0)\n",
    "\n",
    "# Select only the two features we want\n",
    "X = df[['count', 'srv_count']]\n",
    "y = df['binary_label']\n",
    "\n",
    "# Ensure no missing values in selected features\n",
    "X = X.dropna()\n",
    "y = y.loc[X.index]  # Ensure labels align with cleaned features\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Class distribution before balancing:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Balance the dataset\n",
    "print(\"Balancing dataset...\")\n",
    "class_counts = y.value_counts()\n",
    "min_class_count = class_counts.min()\n",
    "\n",
    "class_0 = df[df['binary_label'] == 0].sample(n=min_class_count, random_state=42)\n",
    "class_1 = df[df['binary_label'] == 1].sample(n=min_class_count, random_state=42)\n",
    "\n",
    "balanced_df = pd.concat([class_0, class_1])\n",
    "\n",
    "# Update X and y after balancing\n",
    "X = balanced_df[['count', 'srv_count']]\n",
    "y = balanced_df['binary_label']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def plot_decision_boundary(X, y, model, title):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    # Make predictions for each point in the mesh\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the decision boundary and training points\n",
    "    plt.contourf(xx, yy, Z, alpha=0.4, cmap=plt.cm.coolwarm)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8, edgecolor='k', cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel('Normalized count')\n",
    "    plt.ylabel('Normalized srv_count')\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "# Train and plot Linear SVM\n",
    "print(\"Training Linear SVM...\")\n",
    "linear_svm = SVC(kernel='linear', random_state=42)\n",
    "linear_svm.fit(X_train, y_train)\n",
    "plot_decision_boundary(X_scaled, y, linear_svm, 'Linear SVM Decision Boundary')\n",
    "\n",
    "# Train and plot RBF SVM\n",
    "print(\"Training RBF SVM...\")\n",
    "rbf_svm = SVC(kernel='rbf', random_state=42)\n",
    "rbf_svm.fit(X_train, y_train)\n",
    "plot_decision_boundary(X_scaled, y, rbf_svm, 'RBF SVM Decision Boundary')\n",
    "\n",
    "# Print accuracy scores\n",
    "print(\"\\nLinear SVM accuracy:\", linear_svm.score(X_test, y_test))\n",
    "print(\"RBF SVM accuracy:\", rbf_svm.score(X_test, y_test))\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f1eb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
